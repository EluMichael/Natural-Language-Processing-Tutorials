{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model 0: RNN\n",
    "\n",
    "Given their effectiveness in modeling sequential data, the first acoustic model you will use is an RNN.  As shown in the figure below, the RNN we supply to you will take the time sequence of audio features as input.\n",
    "\n",
    "<img src=\"images/simple_rnn.png\" width=\"50%\">\n",
    "\n",
    "At each time step, the speaker pronounces one of 28 possible characters, including each of the 26 letters in the English alphabet, along with a space character (\" \"), and an apostrophe (').\n",
    "\n",
    "The output of the RNN at each time step is a vector of probabilities with 29 entries, where the $i$-th entry encodes the probability that the $i$-th character is spoken in the time sequence.  (The extra 29th character is an empty \"character\" used to pad training examples within batches containing uneven lengths.)  If you would like to peek under the hood at how characters are mapped to indices in the probability vector, look at the `char_map.py` file in the repository.  The figure below shows an equivalent, rolled depiction of the RNN that shows the output layer in greater detail. \n",
    "\n",
    "<img src=\"images/simple_rnn_unrolled.png\" width=\"60%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Create a GRU layer which returns the sequence and also have implementation mode of 2\n",
    "3. Use a softmax activation function\n",
    "4. Finalize your model using keras.models.Model\n",
    "5. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "6. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model 1: RNN + TimeDistributed Dense\n",
    "\n",
    "Read about the [TimeDistributed](https://keras.io/layers/wrappers/) wrapper and the [BatchNormalization](https://keras.io/layers/normalization/) layer in the Keras documentation.  For your next architecture, you will add [batch normalization](https://arxiv.org/pdf/1510.01378.pdf) to the recurrent layer to reduce training times.  The `TimeDistributed` layer will be used to find more complex patterns in the dataset.  The unrolled snapshot of the architecture is depicted below.\n",
    "\n",
    "<img src=\"images/rnn_model.png\" width=\"60%\">\n",
    "\n",
    "The next figure shows an equivalent, rolled depiction of the RNN that shows the (`TimeDistrbuted`) dense and output layers in greater detail.  \n",
    "\n",
    "<img src=\"images/rnn_model_unrolled.png\" width=\"60%\">\n",
    "\n",
    "Use your research to complete the `rnn_model` function within the `sample_models.py` file.  The function should specify an architecture that satisfies the following requirements:\n",
    "- The first layer of the neural network should be an RNN (`SimpleRNN`, `LSTM`, or `GRU`) that takes the time sequence of audio features as input.  We have added `GRU` units for you, but feel free to change `GRU` to `SimpleRNN` or `LSTM`, if you like!\n",
    "- Whereas the architecture in `simple_rnn_model` treated the RNN output as the final layer of the model, you will use the output of your RNN as a hidden layer.  Use `TimeDistributed` to apply a `Dense` layer to each of the time steps in the RNN output.  Ensure that each `Dense` layer has `output_dim` units.\n",
    "\n",
    "Use the code cell below to load your model into the `model_1` variable.  Use a value for `input_dim` that matches your chosen audio features, and feel free to change the values for `units` and `activation` to tweak the behavior of your recurrent layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Create a GRU layer which returns the sequence and also have implementation mode of 2\n",
    "3. Add batch normalization layer\n",
    "4. Add a TimeDistributed(Dense(output_dim)) layer\n",
    "5. Use a softmax activation function\n",
    "4. Finalize your model using keras.models.Model\n",
    "5. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "6. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model 2: CNN + RNN + TimeDistributed Dense\n",
    "\n",
    "The architecture in `cnn_rnn_model` adds an additional level of complexity, by introducing a [1D convolution layer](https://keras.io/layers/convolutional/#conv1d).  \n",
    "\n",
    "<img src=\"images/cnn_rnn_model.png\" width=\"100%\">\n",
    "\n",
    "This layer incorporates many arguments that can be (optionally) tuned when calling the `cnn_rnn_model` module.  We provide sample starting parameters, which you might find useful if you choose to use spectrogram audio features.  \n",
    "\n",
    "If you instead want to use MFCC features, these arguments will have to be tuned.  Note that the current architecture only supports values of `'same'` or `'valid'` for the `conv_border_mode` argument.\n",
    "\n",
    "When tuning the parameters, be careful not to choose settings that make the convolutional layer overly small.  If the temporal length of the CNN layer is shorter than the length of the transcribed text label, your code will throw an error.\n",
    "\n",
    "Before running the code cell below, you must modify the `cnn_rnn_model` function in `sample_models.py`.  Please add batch normalization to the recurrent layer, and provide the same `TimeDistributed` layer as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Add convolutional with only one dimension (CONV1D) layer\n",
    "3. Add batch normalization layer\n",
    "4. Create a LSTM layer which returns the sequence and also have implementation mode of 2\n",
    "5. Add batch normalization layer\n",
    "6. Add a TimeDistributed(Dense(output_dim)) layer\n",
    "7. Use a softmax activation function\n",
    "8. Finalize your model using keras.models.Model\n",
    "9. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "10. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model 3: Deeper RNN + TimeDistributed Dense\n",
    "\n",
    "Review the code in `rnn_model`, which makes use of a single recurrent layer.  Now, specify an architecture in `deep_rnn_model` that utilizes a variable number `recur_layers` of recurrent layers.  The figure below shows the architecture that should be returned if `recur_layers=2`.  In the figure, the output sequence of the first recurrent layer is used as input for the next recurrent layer.\n",
    "\n",
    "<img src=\"images/deep_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "Feel free to change the supplied values of `units` to whatever you think performs best.  You can change the value of `recur_layers`, as long as your final value is greater than 1. (As a quick check that you have implemented the additional functionality in `deep_rnn_model` correctly, make sure that the architecture that you specify here is identical to `rnn_model` if `recur_layers=1`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Create a LSTM layer which returns the sequence and also have implementation mode of 2\n",
    "3. Add batch normalization layer\n",
    "4. Create a LSTM layer which returns the sequence and also have implementation mode of 2\n",
    "5. Add batch normalization layer\n",
    "6. Add a TimeDistributed(Dense(output_dim)) layer\n",
    "7. Use a softmax activation function\n",
    "8. Finalize your model using keras.models.Model\n",
    "9. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "10. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Model 4: Bidirectional RNN + TimeDistributed Dense\n",
    "\n",
    "Read about the [Bidirectional](https://keras.io/layers/wrappers/) wrapper in the Keras documentation.  For your next architecture, you will specify an architecture that uses a single bidirectional RNN layer, before a (`TimeDistributed`) dense layer.  The added value of a bidirectional RNN is described well in [this paper](http://www.cs.toronto.edu/~hinton/absps/DRNN_speech.pdf).\n",
    "> One shortcoming of conventional RNNs is that they are only able to make use of previous context. In speech recognition, where whole utterances are transcribed at once, there is no reason not to exploit future context as well.  Bidirectional RNNs (BRNNs) do this by processing the data in both directions with two separate hidden layers which are then fed forwards to the same output layer.\n",
    "\n",
    "<img src=\"images/bidirectional_rnn_model.png\" width=\"80%\">\n",
    "\n",
    "Before running the code cell below, you must complete the `bidirectional_rnn_model` function in `sample_models.py`.  Feel free to use `SimpleRNN`, `LSTM`, or `GRU` units.  When specifying the `Bidirectional` wrapper, use `merge_mode='concat'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Create a bidirectional layer which wrapped around a GRU layer which returns the sequence and also have implementation mode of 2.\n",
    "3. Add a TimeDistributed(Dense(output_dim)) layer\n",
    "4. Use a softmax activation function\n",
    "5. Finalize your model using keras.models.Model\n",
    "6. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "7. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Final Model\n",
    "\n",
    "Now that you've tried out many sample models, use what you've learned to draft your own architecture!  While your final acoustic model should not be identical to any of the architectures explored above, you are welcome to merely combine the explored layers above into a deeper architecture.  It is **NOT** necessary to include new layer types that were not explored in the notebook.\n",
    "\n",
    "However, if you would like some ideas for even more layer types, check out these ideas for some additional, optional extensions to your model:\n",
    "\n",
    "- If you notice your model is overfitting to the training dataset, consider adding **dropout**!  To add dropout to [recurrent layers](https://faroit.github.io/keras-docs/1.0.2/layers/recurrent/), pay special attention to the `dropout_W` and `dropout_U` arguments.  This [paper](http://arxiv.org/abs/1512.05287) may also provide some interesting theoretical background.\n",
    "- If you choose to include a convolutional layer in your model, you may get better results by working with **dilated convolutions**.  If you choose to use dilated convolutions, make sure that you are able to accurately calculate the length of the acoustic model's output in the `model.output_length` lambda function.  You can read more about dilated convolutions in Google's [WaveNet paper](https://arxiv.org/abs/1609.03499).  For an example of a speech-to-text system that makes use of dilated convolutions, check out this GitHub [repository](https://github.com/buriburisuri/speech-to-text-wavenet).  You can work with dilated convolutions [in Keras](https://keras.io/layers/convolutional/) by paying special attention to the `padding` argument when you specify a convolutional layer.\n",
    "- If your model makes use of convolutional layers, why not also experiment with adding **max pooling**?  Check out [this paper](https://arxiv.org/pdf/1701.02720.pdf) for example architecture that makes use of max pooling in an acoustic model.\n",
    "- So far, you have experimented with a single bidirectional RNN layer.  Consider stacking the bidirectional layers, to produce a [deep bidirectional RNN](https://www.cs.toronto.edu/~graves/asru_2013.pdf)!\n",
    "\n",
    "All models that you specify in this repository should have `output_length` defined as an attribute.  This attribute is a lambda function that maps the (temporal) length of the input acoustic features to the (temporal) length of the output softmax layer.  This function is used in the computation of CTC loss; to see this, look at the `add_ctc_loss` function in `train_utils.py`.  To see where the `output_length` attribute is defined for the models in the code, take a look at the `sample_models.py` file.  You will notice this line of code within most models:\n",
    "```\n",
    "model.output_length = lambda x: x\n",
    "```\n",
    "The acoustic model that incorporates a convolutional layer (`cnn_rnn_model`) has a line that is a bit different:\n",
    "```\n",
    "model.output_length = lambda x: cnn_output_length(\n",
    "        x, kernel_size, conv_border_mode, conv_stride)\n",
    "```\n",
    "\n",
    "In the case of models that use purely recurrent layers, the lambda function is the identity function, as the recurrent layers do not modify the (temporal) length of their input tensors.  However, convolutional layers are more complicated and require a specialized function (`cnn_output_length` in `sample_models.py`) to determine the temporal length of their output.\n",
    "\n",
    "You will have to add the `output_length` attribute to your final model before running the code cell below.  Feel free to use the `cnn_output_length` function, if it suits your model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**STEP BY STEP WALKTHROUGH:**\n",
    "1. Create your input using keras.layers.Input, Define the shape as (None, input_dim).\n",
    "2. Create a bidirectional layer which wrapped around a GRU layer which returns the sequence and also have implementation mode of 2.\n",
    "3. Add batch normalization layer\n",
    "4. Use a relu activation function\n",
    "5. Create a bidirectional layer which wrapped around a GRU layer which returns the sequence and also have implementation mode of 2.\n",
    "6. Add batch normalization layer\n",
    "7. Use a relu activation function\n",
    "8. Add a TimeDistributed(Dense(output_dim)) layer\n",
    "9. Add softmax activation layer\n",
    "10. Finalize your model using keras.models.Model\n",
    "11. Use **.output_length** to change the output of your neural networks based on the output_dim\n",
    "12. Train the model using train_model that you have imported at the start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "\n",
    "- <a href=\"https://realpython.com/python-speech-recognition/\">The Ultimate Guide To Speech Recognition With Python</a>\n",
    "- <a href=\"https://pythonprogramminglanguage.com/speech-recognition/\">Speech Recognition API</a>\n",
    "- <a href=\"https://www.youtube.com/watch?v=K_WbsFrPUCk\">Speech Recognition using Python - Youtube Video</a>\n",
    "- <a href=\"https://www.edx.org/course/speech-recognition-systems-2\">Speech Recognition Systems Course - Microsoft</a>\n",
    "- <a href=\"https://towardsdatascience.com/automatic-speech-recognition-data-collection-with-youtube-v3-api-mask-rcnn-and-google-vision-api-2370d6776109\">Automatic Speech Recognition Data Collection with Youtube V3 API, Mask-RCNN and Google Vision API</a>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
