{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webinar 2 - Intro to NLP, Text Processing, Spam Classifier\n",
    "In this lession, We will go through the following topics one by one.\n",
    " 1. **Intro to NLP**\n",
    " 2. **Fetching data from a webpage**\n",
    " 3. **Text processing in NLP**\n",
    " 4. **Bayes theorem**\n",
    " 5. **Naive bayes theorem**\n",
    " 5. **Spam Detection using naive bayes theorem**\n",
    " \n",
    "<hr>\n",
    "\n",
    "## 1. Intro to NLP\n",
    "\n",
    "In general we can define a NLP pipeline as geting a raw text, process it, extract relevant features, and build Modes to accomplish various NLP task. We can say that it can be divided into 3 parts:\n",
    "1. **Text processing**\n",
    "2. **Feature extraction**\n",
    "3. **Modeling**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 2. Fetching data from a webpage\n",
    "\n",
    "Let's go to <a href=\"https://techcrunch.com\">techcrunch.com</a> and fetch all of the news titles.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"assets/techcrunch.png\">    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching a webpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the HTML tags\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Getting the titles of each article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the text of first article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the text of all articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "1. <a href=\"https://www.youtube.com/watch?v=aIPqt-OdmS0\">Web scraping and parsing with Beautiful Soup & Python Introduction p.1</a>\n",
    "2. <a href=\"https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe\">How to scrape websites with Python and BeautifulSoup</a>\n",
    "3. <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\">kdnuggets - Text Data Preprocessing: A Walkthrough in Python</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Text Processing\n",
    "\n",
    "In text processing we get the raw data and make it ready for feature extraction. The steps are as follows:\n",
    "1. **Clean the dataset**: Here we will delete all the html tags, if there is some.\n",
    "2. **Normalize the text**: First lowercase the dataset. Second remove punctuation. Third we tokenize the text.\n",
    "3. **Remove stop words**\n",
    "4. **Stemming**: Process of removing the suffixes. For example, “branching”, “branches”, “branched”can all be reduced to “branch”.\n",
    "5. **Lemmatization**: This a technique for reducing words to a normalised form. In here we use a dictionary for mapping different variants of words back to its root. For example, converting words like “was”, “is”, “were” to “be”.\n",
    "\n",
    "Stemming vs lemmatization: The final form of stemming can be meaningless (totally different word) but in lemmatization the final form is also a meaningful word in English. Also stemming doesn’t have a dictionary and its based on some rules but in lemmatization we have dictionary. So stemming less memory intensive option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and \\\n",
    "        definitely watch part 2. It will change your view of the matrix. Are the human people the ones  \\\n",
    "        who started the war ? Is AI a bad thing ?\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Normalize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Tokenizing the text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all stop words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply stemming\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lemmatization on nouns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Lemmatization on verbs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "\n",
    "1. <a href=\"https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\">An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</a> - Chapter 2 Regular Expressions, Text Normalization, Edit Distance\n",
    "2. <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\">kdnuggets - Text Data Preprocessing: A Walkthrough in Python</a>\n",
    "3. <a href=\"https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\"> Natural Language Processing Series - Harrison</a>\n",
    "4. <a href=\"https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\"> Text Preprocessing in Python: Steps, Tools, and Examples </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Bayes Theorem\n",
    "\n",
    "The **Bayes theorem** calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is  composed of a  prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors). \n",
    "\n",
    "##### Let's take a look at example for having a cancer:\n",
    "\n",
    "Now let's find the odds of an individual having cancer, given that he or she was tested for it and got a positive result. \n",
    "Afterward we go on internet and search more about cancer and we figure out that 1% of whole population have cancer. we also figure out that the test correctly predicts 90% of times on those who have cancer (Sensitivity or True Positive Rate.) and it also correctly predicts 90% of times on those who doesn't have cancer. (Specificity or True Negative Rate.)\n",
    "\n",
    "<br>\n",
    "\n",
    "So we assume the following:\n",
    "\n",
    "- `P(Cancer)`: The probability of a person having cancer. It's value is `0.01`  because 1% of whole population have cancer. \n",
    "<br>\n",
    "\n",
    "- `P(Positive)`: The probability of getting a positive test result.\n",
    "<br>\n",
    "\n",
    "- `P(Negative)`: The probability of getting a negative test result.\n",
    "<br>\n",
    "\n",
    "- `P(Positve|Cancer)`: The probability of getting a positive result on a test done for detecting cancer, given that you have cancer. This has a value `0.9` because our test correctly predicts 90% of times on those who have a cancer.\n",
    "<br>\n",
    "\n",
    "- `P(Negative|~Cancer)`: The probability of getting a negative result on a test done for detecting cancer, given that you do not have cancer. This also has a value of `0.9` because our test correctly predicts 90% of times on those who doens't have a cancer.\n",
    "\n",
    "\n",
    "The **Bayes theorem** formula is as follows:\n",
    "\n",
    "<img src=\"assets/Bayes_theorem.png\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The probability of getting a positive test result `P(Positive)` can be calulated using the Sensitivity and Specificity:\n",
    "\n",
    "<img src=\"assets/bayes_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Now let's calculating P(Positive)\n",
    "\n",
    "# P(Cancer)\n",
    "\n",
    "\n",
    "# P(~Cancer)\n",
    "\n",
    "\n",
    "# P(Positive|Cancer) or Sensitivity\n",
    "\n",
    "\n",
    "# P(Negative|~Cancer) or Specieficity\n",
    "\n",
    "\n",
    "# P(Positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Given all of this information we can calculate our **posteriors** as follows:\n",
    "<img src=\"assets/posterior_1.png\">\n",
    "<img src=\"assets/posterior_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating P(Cancer|Positive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculating P(~Cancer|Positive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since: `P(~Cancer|Positive) = (P(~Cancer) * P(Positive|~Cancer) / P(Positive)`\n",
    "\n",
    "Then `P(Positive/~Cancer)` can be computed as `1 - P(Negative/~Cancer)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our result shows that if we get a positive test result, then there is only a 8.33% chance that we actually have cancer and a 91.67% chance that we do not have cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "\n",
    "1. <a href=\"https://classroom.udacity.com/courses/st101\">intro to statistics (Bayes rule)</a> - Udacity free course\n",
    "2. <a href=\"https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/\">An Intuitive (and Short) Explanation of Bayes’ Theorem</a> \n",
    "3. <a href=\"https://www.mathsisfun.com/data/bayes-theorem.html\">Bayes can do magic!</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Naive Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word naive in naive bayes come from the naive assumption we are doing. we always assume that the events are independent from each other. This is a false and naive assumption but in practice it words pretty well.\n",
    "\n",
    "<img src=\"assets/naive assumption.png\">\n",
    "\n",
    "##### Let's take an example in political usage:\n",
    "Assume we have two political candidates in two different parties, 'Jennifer Stewart' of the Gemini Party and 'Gabrielle Jones' of the Liberal Party. Assume that the probablility of Jennifer Stewart giving a speech, `P(Jenifer)` is `0.5` and the same for Gabrielle Jones, `P(Gabrielle) = 0.5`. Below we have the probabilities of each of these candidates saying the words 'liberty', 'war' and 'economy' when they give a speech:\n",
    "\n",
    "- `P(Liberty|Jenifer)` or Probability saying 'liberty' given Jennifer Stewart says it: `0.1`\n",
    "<br>\n",
    "- `P(War|Jenifer)` or Probability saying 'war' given Jennifer Stewart says it: `0.1`\n",
    "<br>\n",
    "- `P(Economy|Jenifer)` or Probability saying 'economy' given Jennifer Stewart says it: `0.8`\n",
    "<br><br>\n",
    "- `P(Liberty|Gabrielle)` or Probability saying 'liberty' given Gabrielle Jones says it: `0.7`\n",
    "<br>\n",
    "- `P(War|Gabrielle)` or Probability saying 'war' given Gabrielle Jones says it: `0.2`\n",
    "<br>\n",
    "- `P(Economy|Gabrielle)` or Probability saying 'economy' given Gabrielle Jones says it: `0.1`\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**Naive assumption:** Given this, what if we had to find the probabilities of Jennifer Stewart saying the words 'liberty' and 'war'? This is where the Naive Bayes'theorem comes into play as we are considering two features, 'liberty' and 'war'.\n",
    "\n",
    "<br><br>\n",
    "Now we are at a place where we can define the <u>formula for the Naive Bayes' theorem</u>:\n",
    "\n",
    "<img src=\"assets/naivebayes.png\" height=\"342\" width=\"342\">\n",
    "\n",
    "- `y` is the class variable or in our case the name of the candidate \n",
    "- `x1` through `xn` are the feature vectors or in our case the individual words. The theorem makes the assumption that each of the feature vectors or words (`xi`) are independent of each other.\n",
    "<br><br>\n",
    "\n",
    "**To break this down, we have to compute the following posterior probabilities:**\n",
    "\n",
    "1. `P(Jenifer|Liberty,War)`: Probability of Jennifer Stewart saying the words liberty and war. Using the formula and our knowledge of Bayes' theorem, we can compute this as follows:\n",
    "    \n",
    "    <img src=\"assets/jenifer_naivebayes.png\">\n",
    "    \n",
    "    Here P(Liberty,War) is the probability of the words 'liberty' and 'war' being said in a speech.\n",
    "    \n",
    "\n",
    "2. `P(Gabrielle|Liberty,War)`: Probability of Gabrielle Jones saying the words liberty and war. Using the formula, we can compute this as follows:\n",
    "    \n",
    "    <img src=\"assets/gabrielle_naivebayes.png\">\n",
    "    \n",
    "    \n",
    "\n",
    "##### Now compute the `P(Liberty,War)` or the probability of the words 'liberty' and 'war' being said in a speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the 3 different probability for Jenifer\n",
    "\n",
    "# P(Jenifer)\n",
    "\n",
    "\n",
    "# P(Libery | Jenifer)\n",
    "\n",
    "\n",
    "# P(War | Jenifer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the 3 different probability for Gabrielle\n",
    "\n",
    "# P(Gabrielle)\n",
    "\n",
    "\n",
    "# P(Liberty | Gabrielle)\n",
    "\n",
    "\n",
    "# P(War | Gabrielle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute P(Liberty , War)\n",
    "\n",
    "# [P(Jenifer) x P(Libery|Jenifer) P(War|Jenifer)] + [P(Gabrielle) x P(Liberty|Gabrielle) x P(War|Gabrielle)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute P(Jenifer | Liberty, War) or the posterior probability of Jenifer\n",
    "\n",
    "# P(Jenifer|Libery,War) = (P(Jenifer) * P(Libery|Jenifer) * P(War|Jenifer)) / P(Libery,War)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute P(Gabrielle | Liberty, War) or the posterior probability of Gabrielle\n",
    "\n",
    "# P(Gabrielle|Libery,War) = (P(Gabrielle) * P(Libery|Gabrielle) * P(War|Gabrielle)) / P(Libery,War)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The sum of our posterior probability should add up to 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "\n",
    "1. <a href=\"https://classroom.udacity.com/courses/ud120\">intro to machine learning (Naive Bayes)</a> - Udacity free course\n",
    "2. <a src=\"https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\">6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R)</a>\n",
    "3. <a src=\"https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\">Naive Bayes Classifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6. Spam Detection Using Naive Bayes Theorem\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag of words features to identify spam e-mail, an approach commonly used in text classification.\n",
    "\n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n",
    "\n",
    "Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s. (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "We will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the shape of data\n",
    "print(\"dataset shape: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Now that we loaded our dataset, Let's convert our labels into a binary number. 0 for ham and 1 for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the labels to binary numbers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "In order to find a model with the highest accuracy possible, We need further testing for our model. For doing so we split our dataset into training and testing set. One for training and another one for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into training and test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction - Bag of Words\n",
    "Now that we have split the data, our next objective is to get the Bag of words and convert our data into the desired matrix format. To do this we will be using CountVectorizer(). The steps are as follows:\n",
    "* First we fir the `CountVectorizer()` into the training data or `X_train`.\n",
    "* Then we transform our testing data or `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the count vectorizer\n",
    "\n",
    "\n",
    "# Fit and transform the training data\n",
    "\n",
    "\n",
    "# Transform the test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the training data in DataFrame\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Sklearn has several Naive Bayes implementations that we can use and so we do not have to do the math from scratch. We will be using sklearns `sklearn.naive_bayes` method.\n",
    "\n",
    "We will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying Naive Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Let's make a prediction on our testing dataset. Later on we will evaluate how well did our model do on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prediction \"0\", \"1\" into \"spam\", \"ham\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a random message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict a correct ham message\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "Now we want to evaluate how well our model is doing. There are various mechanisms for doing so:\n",
    "\n",
    "1. **Accuracy**: `[Correct Predictions/Total Number of Predictions]`\n",
    "\n",
    "2. **Precision**: `[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "3. **Recall(sensitivity)**: `[True Positives/(True Positives + False Negatives)]`\n",
    "\n",
    "4. **F1 score**: weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "\n",
    "<a src=\"http://www.est.uc3m.es/BayesUC3M/Summer_School_UPM/2017/lecture%20notes/Practical1.pdf\">Case Study I: Naive Bayesian spam filtering</a> - Universidad Carlos III de Madrid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
