{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Webinar 2 - Intro to NLP, Text Processing, Spam Classifier\n",
    "In this lession, We will go through the following topics one by one.\n",
    " 1. **Intro to NLP**\n",
    " 2. **Fetching data from a webpage**\n",
    " 3. **Text processing in NLP**\n",
    " 4. **Bayes theorem**\n",
    " 5. **Naive bayes theorem**\n",
    " 5. **Spam Detection using naive bayes theorem**\n",
    " \n",
    "<hr>\n",
    "\n",
    "## 1. Intro to NLP\n",
    "\n",
    "In general we can define a NLP pipeline as geting a raw text, process it, extract relevant features, and build Modes to accomplish various NLP task. We can say that it can be divided into 3 parts:\n",
    "1. **Text processing**\n",
    "2. **Feature extraction**\n",
    "3. **Modeling**\n",
    "\n",
    "<hr>\n",
    "\n",
    "## 2. Fetching data from a webpage\n",
    "\n",
    "Let's go to <a href=\"https://techcrunch.com\">techcrunch.com</a> and fetch all of the news titles.\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"assets/techcrunch.png\">    \n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import request\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching a webpage\n",
    "r = requests.get(\"https://techcrunch.com\")\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the HTML tags\n",
    "soup = BeautifulSoup(r.text, \"html5lib\")\n",
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the titles of each article\n",
    "summaries = soup.find_all(\"h2\", class_=\"post-block__title\")\n",
    "summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Breedr raises £2M led by LocalGlobe for its livestock data and trading platform'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the text of first article\n",
    "summaries[0].find(\"a\").get_text().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Breedr raises £2M led by LocalGlobe for its livestock data and trading platform',\n",
       " 'Google removed 2.3B bad ads, banned ads on 1.5M apps + 28M pages, plans new Policy Manager this year',\n",
       " 'Google is reportedly shutting down its in-house VR film studio',\n",
       " 'MIT’s deflated balloon robot hand can pick up objects 100x its own weight',\n",
       " 'Tiny claws let drones perch like birds and bats',\n",
       " 'Facebook won’t store data in countries with human rights violations — except Singapore',\n",
       " 'The “splinternet” is already here',\n",
       " 'Uber reportedly raising $1B in deal that values self-driving car unit at up to $10B',\n",
       " 'Ford is expanding its self-driving vehicle program to Austin',\n",
       " 'Ysplit wants to make it so you never owe your friends money again',\n",
       " 'Inside Tufts University’s grade-hacking case',\n",
       " 'DoorDash claims drivers made an average of at least $17.50/hour on deliveries in 2018',\n",
       " 'Meet the 19 startups in AngelPad’s 12th batch',\n",
       " 'Apple Music comes to Fire TV',\n",
       " 'Take NVIDIA’s new Deep Learning Robotics Workshop at TC Sessions: Robotics + AI',\n",
       " 'Opportunity’s last Mars panorama is a showstopper',\n",
       " 'Boeing requests FAA ground the 737 as the president pushes an emergency order',\n",
       " 'Sketch, maker of popular design tools, just landed $20 million in Series A funding from Benchmark in its first outside round',\n",
       " 'The adversarial persuasion machine: a conversation with James Williams',\n",
       " 'The FDA proposes further restrictions to sales of flavored e-cig products']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the text of all articles\n",
    "articles = [i_article.find(\"a\").get_text().strip() for i_article in summaries]\n",
    "articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "1. <a href=\"https://www.youtube.com/watch?v=aIPqt-OdmS0\">Web scraping and parsing with Beautiful Soup & Python Introduction p.1</a>\n",
    "2. <a href=\"https://medium.freecodecamp.org/how-to-scrape-websites-with-python-and-beautifulsoup-5946935d93fe\">How to scrape websites with Python and BeautifulSoup</a>\n",
    "3. <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\">kdnuggets - Text Data Preprocessing: A Walkthrough in Python</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 3. Text Processing\n",
    "\n",
    "In text processing we get the raw data and make it ready for feature extraction. The steps are as follows:\n",
    "1. **Clean the dataset**: Here we will delete all the html tags, if there is some.\n",
    "2. **Normalize the text**: First lowercase the dataset. Second remove punctuation. Third we tokenize the text.\n",
    "3. **Remove stop words**\n",
    "4. **Stemming**: Process of removing the suffixes. For example, “branching”, “branches”, “branched”can all be reduced to “branch”.\n",
    "5. **Lemmatization**: This a technique for reducing words to a normalised form. In here we use a dictionary for mapping different variants of words back to its root. For example, converting words like “was”, “is”, “were” to “be”.\n",
    "\n",
    "Stemming vs lemmatization: The final form of stemming can be meaningless (totally different word) but in lemmatization the final form is also a meaningful word in English. Also stemming doesn’t have a dictionary and its based on some rules but in lemmatization we have dictionary. So stemming less memory intensive option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Load and cleaning the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first time you see The Second Renaissance it may look boring. Look at it at least twice and         definitely watch part 2. It will change your view of the matrix. Are the human people the ones          who started the war ? Is AI a bad thing ?\n"
     ]
    }
   ],
   "source": [
    "text = \"The first time you see The Second Renaissance it may look boring. Look at it at least twice and \\\n",
    "        definitely watch part 2. It will change your view of the matrix. Are the human people the ones  \\\n",
    "        who started the war ? Is AI a bad thing ?\"\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Normalize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first time you see the second renaissance it may look boring. look at it at least twice and         definitely watch part 2. it will change your view of the matrix. are the human people the ones          who started the war ? is ai a bad thing ?\n"
     ]
    }
   ],
   "source": [
    "# Lower casing the dataset\n",
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the first time you see the second renaissance it may look boring  look at it at least twice and         definitely watch part 2  it will change your view of the matrix  are the human people the ones          who started the war   is ai a bad thing  \n"
     ]
    }
   ],
   "source": [
    "# Remove punctuation\n",
    "text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'first', 'time', 'you', 'see', 'the', 'second', 'renaissance', 'it', 'may', 'look', 'boring', 'look', 'at', 'it', 'at', 'least', 'twice', 'and', 'definitely', 'watch', 'part', '2', 'it', 'will', 'change', 'your', 'view', 'of', 'the', 'matrix', 'are', 'the', 'human', 'people', 'the', 'ones', 'who', 'started', 'the', 'war', 'is', 'ai', 'a', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing the text\n",
    "text = word_tokenize(text)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'ones', 'started', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "text = [i_word for i_word in text if i_word not in stopwords.words(\"english\")]\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaiss', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definit', 'watch', 'part', '2', 'chang', 'view', 'matrix', 'human', 'peopl', 'one', 'start', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "stemmed = [PorterStemmer().stem(i_word) for i_word in text]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'boring', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'one', 'started', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "lemmatized = [WordNetLemmatizer().lemmatize(i_word, pos='n') for i_word in text]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['first', 'time', 'see', 'second', 'renaissance', 'may', 'look', 'bore', 'look', 'least', 'twice', 'definitely', 'watch', 'part', '2', 'change', 'view', 'matrix', 'human', 'people', 'one', 'start', 'war', 'ai', 'bad', 'thing']\n"
     ]
    }
   ],
   "source": [
    "lemmatized = [WordNetLemmatizer().lemmatize(i_word, pos='v') for i_word in lemmatized]\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "\n",
    "1. <a href=\"https://web.stanford.edu/~jurafsky/slp3/ed3book.pdf\">An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition</a> - Chapter 2 Regular Expressions, Text Normalization, Edit Distance\n",
    "2. <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\">kdnuggets - Text Data Preprocessing: A Walkthrough in Python</a>\n",
    "3. <a href=\"https://pythonprogramming.net/tokenizing-words-sentences-nltk-tutorial/\"> Natural Language Processing Series - Harrison</a>\n",
    "4. <a href=\"https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\"> Text Preprocessing in Python: Steps, Tools, and Examples </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 4. Bayes Theorem\n",
    "\n",
    "The **Bayes theorem** calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is  composed of a  prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors). \n",
    "\n",
    "##### Let's take a look at example for having a cancer:\n",
    "\n",
    "Now let's find the odds of an individual having cancer, given that he or she was tested for it and got a positive result. \n",
    "Afterward we go on internet and search more about cancer and we figure out that 1% of whole population have cancer. we also figure out that the test correctly predicts 90% of times on those who have cancer (Sensitivity or True Positive Rate.) and it also correctly predicts 90% of times on those who doesn't have cancer. (Specificity or True Negative Rate.)\n",
    "\n",
    "<br>\n",
    "\n",
    "So we assume the following:\n",
    "\n",
    "- `P(Cancer)`: The probability of a person having cancer. It's value is `0.01`  because 1% of whole population have cancer. \n",
    "<br>\n",
    "\n",
    "- `P(Positive)`: The probability of getting a positive test result.\n",
    "<br>\n",
    "\n",
    "- `P(Negative)`: The probability of getting a negative test result.\n",
    "<br>\n",
    "\n",
    "- `P(Positve|Cancer)`: The probability of getting a positive result on a test done for detecting cancer, given that you have cancer. This has a value `0.9` because our test correctly predicts 90% of times on those who have a cancer.\n",
    "<br>\n",
    "\n",
    "- `P(Negative|~Cancer)`: The probability of getting a negative result on a test done for detecting cancer, given that you do not have cancer. This also has a value of `0.9` because our test correctly predicts 90% of times on those who doens't have a cancer.\n",
    "\n",
    "\n",
    "The **Bayes theorem** formula is as follows:\n",
    "\n",
    "<img src=\"assets/Bayes_theorem.png\">\n",
    "\n",
    "<br><br>\n",
    "\n",
    "The probability of getting a positive test result `P(Positive)` can be calulated using the Sensitivity and Specificity:\n",
    "\n",
    "<img src=\"assets/bayes_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Positve) or probability of getting a positive test result is equal to: 10.80%\n"
     ]
    }
   ],
   "source": [
    "### Now let's calculating P(Positive)\n",
    "\n",
    "# P(Cancer)\n",
    "p_cancer = 0.01\n",
    "\n",
    "# P(~Cancer)\n",
    "p_no_cancer = 1-0.01\n",
    "\n",
    "# P(Positive|Cancer) or Sensitivity\n",
    "p_positive_cancer = 0.9\n",
    "\n",
    "# P(Negative|~Cancer) or Specieficity\n",
    "p_negative_no_cancer = 0.9\n",
    "\n",
    "# P(Positive)\n",
    "p_positive = (p_cancer * p_positive_cancer) + (p_no_cancer * (1-p_negative_no_cancer))\n",
    "print(\"P(Positve) or probability of getting a positive test result is equal to: {:.2f}%\".format(p_positive*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Given all of this information we can calculate our **posteriors** as follows:\n",
    "<img src=\"assets/posterior_1.png\">\n",
    "<img src=\"assets/posterior_2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Cancer|Positive) or Probability of having cancer given that the test is positive: 8.33%\n"
     ]
    }
   ],
   "source": [
    "### Calculating P(Cancer|Positive)\n",
    "\n",
    "p_cancer_positive = (p_positive_cancer * p_cancer) / p_positive\n",
    "print(\"P(Cancer|Positive) or Probability of having cancer given that the test is positive: {:.2f}%\".format(p_cancer_positive*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(~Cancer|Positive) or Probability of not having cancer given that the test is positive: 91.67%\n"
     ]
    }
   ],
   "source": [
    "### Calculating P(~Cancer|Positive)\n",
    "\n",
    "p_pos_no_cancer = 1 - p_negative_no_cancer\n",
    "\n",
    "p_no_cancer_positive = (p_no_cancer * p_pos_no_cancer) / p_positive\n",
    "\n",
    "print(\"P(~Cancer|Positive) or Probability of not having cancer given that the test is positive: {:.2f}%\".format(p_no_cancer_positive*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that since: `P(~Cancer|Positive) = (P(~Cancer) * P(Positive|~Cancer) / P(Positive)`\n",
    "\n",
    "Then `P(Positive/~Cancer)` can be computed as `1 - P(Negative/~Cancer)`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Our result shows that if we get a positive test result, then there is only a 8.33% chance that we actually have cancer and a 91.67% chance that we do not have cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "\n",
    "1. <a href=\"https://classroom.udacity.com/courses/st101\">intro to statistics (Bayes rule)</a> - Udacity free course\n",
    "2. <a href=\"https://betterexplained.com/articles/an-intuitive-and-short-explanation-of-bayes-theorem/\">An Intuitive (and Short) Explanation of Bayes’ Theorem</a> \n",
    "3. <a href=\"https://www.mathsisfun.com/data/bayes-theorem.html\">Bayes can do magic!</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 5. Naive Bayes Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word naive in naive bayes come from the naive assumption we are doing. we always assume that the events are independent from each other. This is a false and naive assumption but in practice it words pretty well.\n",
    "\n",
    "<img src=\"assets/naive assumption.png\">\n",
    "\n",
    "##### Let's take an example in political usage:\n",
    "Assume we have two political candidates in two different parties, 'Jennifer Stewart' of the Gemini Party and 'Gabrielle Jones' of the Liberal Party. Assume that the probablility of Jennifer Stewart giving a speech, `P(Jenifer)` is `0.5` and the same for Gabrielle Jones, `P(Gabrielle) = 0.5`. Below we have the probabilities of each of these candidates saying the words 'liberty', 'war' and 'economy' when they give a speech:\n",
    "\n",
    "- `P(Liberty|Jenifer)` or Probability saying 'liberty' given Jennifer Stewart says it: `0.1`\n",
    "<br>\n",
    "- `P(War|Jenifer)` or Probability saying 'war' given Jennifer Stewart says it: `0.1`\n",
    "<br>\n",
    "- `P(Economy|Jenifer)` or Probability saying 'economy' given Jennifer Stewart says it: `0.8`\n",
    "<br><br>\n",
    "- `P(Liberty|Gabrielle)` or Probability saying 'liberty' given Gabrielle Jones says it: `0.7`\n",
    "<br>\n",
    "- `P(War|Gabrielle)` or Probability saying 'war' given Gabrielle Jones says it: `0.2`\n",
    "<br>\n",
    "- `P(Economy|Gabrielle)` or Probability saying 'economy' given Gabrielle Jones says it: `0.1`\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "**Naive assumption:** Given this, what if we had to find the probabilities of Jennifer Stewart saying the words 'liberty' and 'war'? This is where the Naive Bayes'theorem comes into play as we are considering two features, 'liberty' and 'war'.\n",
    "\n",
    "<br><br>\n",
    "Now we are at a place where we can define the <u>formula for the Naive Bayes' theorem</u>:\n",
    "\n",
    "<img src=\"assets/naivebayes.png\" height=\"342\" width=\"342\">\n",
    "\n",
    "- `y` is the class variable or in our case the name of the candidate \n",
    "- `x1` through `xn` are the feature vectors or in our case the individual words. The theorem makes the assumption that each of the feature vectors or words (`xi`) are independent of each other.\n",
    "<br><br>\n",
    "\n",
    "**To break this down, we have to compute the following posterior probabilities:**\n",
    "\n",
    "1. `P(Jenifer|Liberty,War)`: Probability of Jennifer Stewart saying the words liberty and war. Using the formula and our knowledge of Bayes' theorem, we can compute this as follows:\n",
    "    \n",
    "    <img src=\"assets/jenifer_naivebayes.png\">\n",
    "    \n",
    "    Here P(Liberty,War) is the probability of the words 'liberty' and 'war' being said in a speech.\n",
    "    \n",
    "\n",
    "2. `P(Gabrielle|Liberty,War)`: Probability of Gabrielle Jones saying the words liberty and war. Using the formula, we can compute this as follows:\n",
    "    \n",
    "    <img src=\"assets/gabrielle_naivebayes.png\">\n",
    "    \n",
    "    \n",
    "\n",
    "##### Now compute the `P(Liberty,War)` or the probability of the words 'liberty' and 'war' being said in a speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the 3 different probability for Jenifer\n",
    "\n",
    "# P(Jenifer)\n",
    "p_jenifer = 0.5\n",
    "\n",
    "# P(Libery | Jenifer)\n",
    "p_liberty_jenifer = 0.1\n",
    "\n",
    "# P(War | Jenifer)\n",
    "p_war_jenifer = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Computing the 3 different probability for Gabrielle\n",
    "\n",
    "# P(Gabrielle)\n",
    "p_gabrielle = 0.5\n",
    "\n",
    "# P(Liberty | Gabrielle)\n",
    "p_liberty_gabrielle = 0.7\n",
    "\n",
    "# P(War | Gabrielle)\n",
    "p_war_gabrielle = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prbability of the words 'liberty' and 'war' being said in a speech: 7.50%\n"
     ]
    }
   ],
   "source": [
    "### Compute P(Liberty , War)\n",
    "\n",
    "# [P(Jenifer) x P(Libery|Jenifer) P(War|Jenifer)] + [P(Gabrielle) x P(Liberty|Gabrielle) x P(War|Gabrielle)]\n",
    "p_liberty_war = (p_jenifer * p_liberty_jenifer * p_war_jenifer) + (p_gabrielle * p_liberty_gabrielle * p_war_gabrielle)\n",
    "\n",
    "print(\"Prbability of the words 'liberty' and 'war' being said in a speech: {:.2f}%\".format(p_liberty_war*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability the Jenifer saying the word liberty and war is 6.67%\n"
     ]
    }
   ],
   "source": [
    "### Compute P(Jenifer | Liberty, War) or the posterior probability of Jenifer\n",
    "\n",
    "# P(Jenifer|Libery,War) = (P(Jenifer) * P(Libery|Jenifer) * P(War|Jenifer)) / P(Libery,War)\n",
    "p_jenifer_liberty_war = (p_jenifer * p_liberty_jenifer * p_war_jenifer) / p_liberty_war\n",
    "\n",
    "print(\"The probability the Jenifer saying the word liberty and war is {:.2f}%\".format(p_jenifer_liberty_war*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability the Gabrielle saying the word liberty and war is 93.33%\n"
     ]
    }
   ],
   "source": [
    "### Compute P(Gabrielle | Liberty, War) or the posterior probability of Gabrielle\n",
    "\n",
    "# P(Gabrielle|Libery,War) = (P(Gabrielle) * P(Libery|Gabrielle) * P(War|Gabrielle)) / P(Libery,War)\n",
    "p_gabrielle_liberty_war = (p_gabrielle * p_liberty_gabrielle * p_war_gabrielle) / p_liberty_war\n",
    "\n",
    "print(\"The probability the Gabrielle saying the word liberty and war is {:.2f}%\".format(p_gabrielle_liberty_war*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: The sum of our posterior probability should add up to 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Further Resources:**\n",
    "\n",
    "1. <a href=\"https://classroom.udacity.com/courses/ud120\">intro to machine learning (Naive Bayes)</a> - Udacity free course\n",
    "2. <a src=\"https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/\">6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R)</a>\n",
    "3. <a src=\"https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c\">Naive Bayes Classifier</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## 6. Spam Detection Using Naive Bayes Theorem\n",
    "\n",
    "### Introduction \n",
    "\n",
    "Naive Bayes classifiers are a popular statistical technique of e-mail filtering. They typically use bag of words features to identify spam e-mail, an approach commonly used in text classification.\n",
    "\n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.\n",
    "\n",
    "Naive Bayes spam filtering is a baseline technique for dealing with spam that can tailor itself to the email needs of individual users and give low false positive spam detection rates that are generally acceptable to users. It is one of the oldest ways of doing spam filtering, with roots in the 1990s. (Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "We will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>spam</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ham</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>spam</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>spam</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                        sms_message\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
       "5  spam  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6   ham  Even my brother is not like to speak with me. ...\n",
       "7   ham  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8  spam  WINNER!! As a valued network customer you have...\n",
       "9  spam  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "dataset = pd.read_table(\"./dataset/SMSSpamCollection\", \n",
    "                        sep = \"\\t\", \n",
    "                        header = None, \n",
    "                        names = [\"label\", \"sms_message\"])\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset shape:  (5572, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"dataset shape: \", dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "Now that we loaded our dataset, Let's convert our labels into a binary number. 0 for ham and 1 for spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>As per your request 'Melle Melle (Oru Minnamin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>WINNER!! As a valued network customer you have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>Had your mobile 11 months or more? U R entitle...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                        sms_message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...\n",
       "5      1  FreeMsg Hey there darling it's been 3 week's n...\n",
       "6      0  Even my brother is not like to speak with me. ...\n",
       "7      0  As per your request 'Melle Melle (Oru Minnamin...\n",
       "8      1  WINNER!! As a valued network customer you have...\n",
       "9      1  Had your mobile 11 months or more? U R entitle..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the labels to binary numbers\n",
    "dataset[\"label\"] = dataset[\"label\"].map({\"ham\": 0, \"spam\": 1})\n",
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Dataset\n",
    "In order to find a model with the highest accuracy possible, We need further testing for our model. For doing so we split our dataset into training and testing set. One for training and another one for testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: \n",
      " 4179\n",
      "Test set size: \n",
      " 1393\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset[\"sms_message\"],\n",
    "                                                    dataset[\"label\"],\n",
    "                                                    test_size =0.25,\n",
    "                                                    random_state = 1)\n",
    "\n",
    "print(\"Training set size: \\n\", X_train.shape[0])\n",
    "print(\"Test set size: \\n\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction - Bag of Words\n",
    "Now that we have split the data, our next objective is to get the Bag of words and convert our data into the desired matrix format. To do this we will be using CountVectorizer(). The steps are as follows:\n",
    "* First we fir the `CountVectorizer()` into the training data or `X_train`.\n",
    "* Then we transform our testing data or `X_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the count vectorizer\n",
    "count_vector = CountVectorizer(lowercase = True, \n",
    "                               token_pattern = \"(?u)\\\\b\\\\w\\\\w+\\\\b\",\n",
    "                               stop_words = \"english\")\n",
    "\n",
    "# Fit and transform the training data\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "#print(\"Vocabulary in Training Set: \\n\", count_vector.get_feature_names(), \"\\n\\n\")\n",
    "\n",
    "# Transform the test data\n",
    "test_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>0207</th>\n",
       "      <th>02072069400</th>\n",
       "      <th>...</th>\n",
       "      <th>zed</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>èn</th>\n",
       "      <th>〨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 7204 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  008704050406  0121  01223585236  01223585334  0125698789  02  \\\n",
       "0   0    0             0     0            0            0           0   0   \n",
       "1   0    0             0     0            0            0           0   0   \n",
       "2   0    0             0     0            0            0           0   0   \n",
       "3   0    0             0     0            0            1           0   0   \n",
       "4   0    0             0     0            0            0           0   0   \n",
       "5   0    0             0     0            0            0           0   0   \n",
       "6   0    0             0     0            0            0           0   0   \n",
       "7   0    0             0     0            0            0           0   0   \n",
       "8   0    0             0     0            0            0           0   0   \n",
       "9   0    0             0     0            0            0           0   0   \n",
       "\n",
       "   0207  02072069400 ...   zed  zeros  zhong  zindgi  zoe  zoom  zouk  zyada  \\\n",
       "0     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "1     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "2     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "3     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "4     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "5     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "6     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "7     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "8     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "9     0            0 ...     0      0      0       0    0     0     0      0   \n",
       "\n",
       "   èn  〨ud  \n",
       "0   0    0  \n",
       "1   0    0  \n",
       "2   0    0  \n",
       "3   0    0  \n",
       "4   0    0  \n",
       "5   0    0  \n",
       "6   0    0  \n",
       "7   0    0  \n",
       "8   0    0  \n",
       "9   0    0  \n",
       "\n",
       "[10 rows x 7204 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the training data in DataFrame\n",
    "pd.DataFrame(data = training_data.toarray(), \n",
    "             columns = count_vector.get_feature_names()).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "\n",
    "Sklearn has several Naive Bayes implementations that we can use and so we do not have to do the math from scratch. We will be using sklearns `sklearn.naive_bayes` method.\n",
    "\n",
    "We will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Naive Bayes\n",
    "naive_bayes = MultinomialNB()\n",
    "\n",
    "naive_bayes.fit(training_data, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "Let's make a prediction on our testing dataset. Later on we will evaluate how well did our model do on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:  [0 0 0 ... 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Predicting the test set\n",
    "predictions = naive_bayes.predict(test_data)\n",
    "print(\"predictions: \", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the prediction \"0\", \"1\" into \"spam\", \"ham\"\n",
    "def pred_num_to_text(prediction):\n",
    "    if prediction == 1:\n",
    "        return \"spam\"\n",
    "    elif prediction == 0:\n",
    "        return \"ham\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recieved message: \n",
      "\"FreeMsg Why haven't you replied to my text? I'm Randy, sexy, female and live local. Luv to hear from u. Netcollex Ltd 08700621170150p per msg reply Stop to end\"\n",
      "\n",
      "Prediction: \n",
      " spam\n"
     ]
    }
   ],
   "source": [
    "id_msg = 16\n",
    "\n",
    "print(\"The recieved message: \\n\\\"\" + X_test.iloc[id_msg] + \"\\\"\\n\")\n",
    "print(\"Prediction: \\n\", pred_num_to_text(naive_bayes.predict(test_data[id_msg])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The recieved message: \n",
      "\"I want kfc its Tuesday. Only buy 2 meals ONLY 2. No gravy. Only 2 Mark. 2!\"\n",
      "\n",
      "Prediction: \n",
      " ham\n"
     ]
    }
   ],
   "source": [
    "id_msg = 6\n",
    "\n",
    "print(\"The recieved message: \\n\\\"\" + X_test.iloc[id_msg] + \"\\\"\\n\")\n",
    "print(\"Prediction: \\n\", pred_num_to_text(naive_bayes.predict(test_data[id_msg])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model\n",
    "\n",
    "Now we want to evaluate how well our model is doing. There are various mechanisms for doing so:\n",
    "\n",
    "1. **Accuracy**: `[Correct Predictions/Total Number of Predictions]`\n",
    "\n",
    "2. **Precision**: `[True Positives/(True Positives + False Positives)]`\n",
    "\n",
    "3. **Recall(sensitivity)**: `[True Positives/(True Positives + False Negatives)]`\n",
    "\n",
    "4. **F1 score**: weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.78%\n",
      "Precision Score: 96.15%\n",
      "Recall Score: 94.59%\n",
      "F1 Score: 95.37%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, predictions)*100))\n",
    "print(\"Precision Score: {:.2f}%\".format(precision_score(y_test, predictions)*100))\n",
    "print(\"Recall Score: {:.2f}%\".format(recall_score(y_test, predictions)*100))\n",
    "print(\"F1 Score: {:.2f}%\".format(f1_score(y_test, predictions)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resources:**\n",
    "\n",
    "<a src=\"http://www.est.uc3m.es/BayesUC3M/Summer_School_UPM/2017/lecture%20notes/Practical1.pdf\">Case Study I: Naive Bayesian spam filtering</a> - Universidad Carlos III de Madrid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
