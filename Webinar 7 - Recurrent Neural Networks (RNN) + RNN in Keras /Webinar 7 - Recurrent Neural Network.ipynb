{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) - NumPy\n",
    "\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implementing activation function of softmax.\n",
    "    \"\"\"\n",
    "    # TODO: Write the softmx formula here\n",
    "    return sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward propagation for the basic Recurrent Neural NetworkÂ¶\n",
    " A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RNN cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.\n",
    "\n",
    "##### The whole network:\n",
    "<img src = \"./assets/RNN_Cell_1.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>\n",
    " \n",
    "##### Zooming in one cell:\n",
    "<img src = \"./assets/RNN_Cell_2.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>\n",
    " \n",
    "##### One cell in greater detail:\n",
    "<img src = \"./assets/RNN_Cell_3.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for a single forward step for RNN cell\n",
    "def rnn_cell_forward(Xt, St_prev, parameters_dict):\n",
    "    \"\"\"\n",
    "    Implementing a single forward step for the RNN cell.\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    \n",
    "    - Xt: Our input data at timestep \"t\". It should be numpy array of shape (n_x, m).\n",
    "    \n",
    "    - St_prev: Hidden state or state at timestep \"t-1\". It should be numpy array of shape (n_s, m).\n",
    "    \n",
    "    - parameters_dict: Python dictionary containing:\n",
    "                           . Wx: A weight matrix connecting the inputs to the state (or hidden state).\n",
    "                                 It should be numpy array of shape (n_s, n_x).  \n",
    "                           . Ws: A weight matrix connecting the state (or hidden state) from previous timestep to the state (or hidden state) in the current timestep.\n",
    "                                 It should be numpy array of shape (n_s, n_s).\n",
    "                           . Wy: A weight matrix connecting the state (or hidden state) to the output layer.\n",
    "                                 It should be numpy array of shape (n_y, n_s).\n",
    "                           . bs: Bias for state. \n",
    "                                 It should be numpy array of shape (n_s, 1).\n",
    "                           . by: Bias for output layer. \n",
    "                                 It should be numpy array of shape (n_y, 1).\n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - St_next: Next state (or hidden state).\n",
    "               It should be numpy array of shape (n_s, m).\n",
    "               \n",
    "    - Yt: Output at timestep \"t\".\n",
    "          It should be numpy array of shape (n_y, m).\n",
    "          \n",
    "    - cache: Tuple of values needed for the backward pass. contains (St_next, St_prev, Xt, parameters_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrive parameters\n",
    "    Wx = # TODO: retrieve Wx\n",
    "    Ws = # TODO: retrieve Ws\n",
    "    Wy = # TODO: retrieve Wy\n",
    "    bs = # TODO: retrieve bs\n",
    "    by = # TODO: retrieve by\n",
    "    \n",
    "    # Compute the next activation state\n",
    "    St_next = # TODO: compute hidden state at time t+!\n",
    "    \n",
    "    # Compute output of current cell\n",
    "    Yt = # TODO: compute output\n",
    "    \n",
    "    # Store variables for backpropagation\n",
    "    cache = # TODO: Keep hidden state at t, hidden state at t-1, input data at t, parameters_dict in a tuple\n",
    "    \n",
    "    return St_next, Yt, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "Xt = np.random.randn(3, 10)\n",
    "\n",
    "# Hidden state at time t-1\n",
    "St_prev = np.random.randn(5, 10)\n",
    "\n",
    "# Weight matrices\n",
    "Ws = np.random.randn(5, 5)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wy = np.random.randn(2, 5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Ws\": Ws, \"Wx\": Wx, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute the forward step for the RNN cell.\n",
    "St_next, Yt, cache = # TODO: compute forward step for rnn cell\n",
    "\n",
    "# Get the shape of hidden state at time t\n",
    "print(\"Shape of hidden state at time t: \", St_next.shape, \"\\n\")\n",
    "\n",
    "# Get the shape of output\n",
    "print(\"Output shape: \", Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RNN forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell and the current time-step's input data. It outputs a hidden state and a prediction for this time-step.\n",
    "\n",
    "<img src = \"./assets/RNN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for forward pass of RNN\n",
    "def rnn_forward(X, S_0, parameters_dict):\n",
    "    \"\"\"\n",
    "    Implementing the forward pass of the recurrent neural network (RNN).\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - X: Input data for every time-step.\n",
    "         It should be numpy array of shape (n_x, m, T_x). T_x is total length of x.\n",
    "         \n",
    "    - S_0: Initial hidden state.\n",
    "           It should be numpy array of shape (n_s, m).\n",
    "                 \n",
    "    - parameters_dict: Python dictionary containing:\n",
    "                           . Wx: A weight matrix connecting the inputs to the state (or hidden state).\n",
    "                                 It should be numpy array of shape (n_s, n_x).  \n",
    "                           . Ws: A weight matrix connecting the state (or hidden state) from previous timestep to the state (or hidden state) in the current timestep.\n",
    "                                 It should be numpy array of shape (n_s, n_s).\n",
    "                           . Wy: A weight matrix connecting the state (or hidden state) to the output layer.\n",
    "                                 It should be numpy array of shape (n_y, n_s).\n",
    "                           . bs: Bias for state. \n",
    "                                 It should be numpy array of shape (n_s, 1).\n",
    "                           . by: Bias for output layer. \n",
    "                                 It should be numpy array of shape (n_y, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - S: Hidden states for every time-step.\n",
    "         It should be numpy array of shape (n_s, m, T_x).\n",
    "         \n",
    "    - y_pred: Predictions for every time-step.\n",
    "              It should be numpy array of shape (n_y, m, T_x).\n",
    "          \n",
    "    - caches: Tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrive dimensions\n",
    "    n_x, m, T_x = # TODO: Get the input shape\n",
    "    n_y, n_s = # TODO: Get the shape of Wy\n",
    "    \n",
    "    # Initialize state \"S\" and predicted \"y\" with zeros\n",
    "    S = # TODO: initialize state with zeos\n",
    "    y_pred = # TODO: initialize output prediction with zeros\n",
    "    \n",
    "    # Initialize S_next\n",
    "    S_next = S_0\n",
    "    \n",
    "    # Initialize caches which contains all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Loop over all timesteps\n",
    "    for t in range(T_x):\n",
    "        \n",
    "        # TODO: Updating next state, Predicting the output, And getting the cache\n",
    "        \n",
    "        # TODO: Save the next state into hidden state\n",
    "        \n",
    "        # TODO: Save the value of prediction into y\n",
    "        \n",
    "        # TODO: Append \"cache\" into \"caches\"\n",
    "        \n",
    "    # Store values needed for backward propagation in cache\n",
    "    caches = (caches, X)\n",
    "    \n",
    "    return S, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "X = np.random.randn(3,10,4)\n",
    "\n",
    "# Initial hidden state\n",
    "S_0 = np.random.randn(5,10)\n",
    "\n",
    "# Weight matrices\n",
    "Ws = np.random.randn(5,5)\n",
    "Wx = np.random.randn(5,3)\n",
    "Wy = np.random.randn(2,5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Ws\": Ws, \"Wx\": Wx, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "S, y_pred, caches = # TODO: compute forward pass of the RNN\n",
    "\n",
    "# Get the shape of hidden state\n",
    "print(\"Shape of hidden state:\", S.shape, \"\\n\")\n",
    "\n",
    "# Get the shape of predicted y\n",
    "print(\"Shape of predicted y: \", y_pred.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation in recurrent neural networks\n",
    "\n",
    "In modern deep learning frameworks, we only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. \n",
    "\n",
    "In a simple (fully connected) neural network, we did backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks we can to calculate the derivatives with respect to the cost in order to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style = \"width:650px\" src = \"./assets/BBTT.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Basic RNN cell backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the backward pass of RNN cell\n",
    "def rnn_cell_backward(ds_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for RNN cell (single timestep)\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - ds_next: Gradient of loss with respect to next hidden state.\n",
    "    \n",
    "    - cache: A dictionary containing the output of rnn_cell_forward()\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - gradients: A dictionary which contains the following.\n",
    "                     . dx: Gradients of input data.\n",
    "                           It should be numpy array of shape (n_x, m)\n",
    "                     . ds_prev: Gradients of previous hidden state.\n",
    "                                It should be numpy array of shape (n_s, m)\n",
    "                     . dWx: Gradients of input-to-hidden weights. \n",
    "                            It should be numpy array of shape (n_s, n_x).\n",
    "                     . dWs: Gradients of hidden-to-hidden weights.\n",
    "                            It should be numpy array of shape (n_s, n_s).\n",
    "                     . dbs: Gradients of bias vector. \n",
    "                            It should be numpy array of shape (n_s, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # TODO: Retrieve values from cache\n",
    "    \n",
    "    # Retrive parameters\n",
    "    Wx = # TODO: retrieve Wx\n",
    "    Ws = # TODO: retrieve Ws\n",
    "    Wy = # TODO: retrieve Wy\n",
    "    bs = # TODO: retrieve bs\n",
    "    by = # TODO: retrieve by\n",
    "    \n",
    "    # Compute the gradient of tanh with respect to S_next\n",
    "    dtanh = # TODO\n",
    "    \n",
    "    # Compute the gradient of loss with repect to Wx\n",
    "    dXt = # TODO\n",
    "    dWx = # TODO\n",
    "    \n",
    "    # Compute the gradient with respect to Ws\n",
    "    ds_prev = # TODO\n",
    "    dWs = # TODO\n",
    "    \n",
    "    # Compute the gradient with respect b\n",
    "    dbs = # TODO\n",
    "    \n",
    "    # Store the gradients\n",
    "    gradients = {\"dXt\": dXt, \"ds_prev\": ds_prev, \"dWx\": dWx, \"dWs\": dWs, \"dbs\": dbs}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "Xt = np.random.randn(3,10)\n",
    "\n",
    "# Previous hidden state\n",
    "S_prev = np.random.randn(5,10)\n",
    "\n",
    "# Weight matrices\n",
    "Wx = np.random.randn(5,3)\n",
    "Ws = np.random.randn(5,5)\n",
    "Wy = np.random.randn(2,5)\n",
    "\n",
    "# Biases\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Wx\": Wx, \"Ws\": Ws, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute single forward step for RNN cell\n",
    "S_next, Yt, cache = # TODO\n",
    "\n",
    "# Gradient of loss with respect to next hidden state.\n",
    "ds_next = np.random.randn(5,10)\n",
    "\n",
    "# Compute backward pass of the RNN cell\n",
    "gradients = # TODO\n",
    "\n",
    "# Get the shapes\n",
    "print(\"gradients[\\\"dXt\\\"].shape =\", gradients[\"dXt\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"ds_prev\\\"].shape =\", gradients[\"ds_prev\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dWx\\\"].shape =\", gradients[\"dWx\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dWs\\\"].shape =\", gradients[\"dWs\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dbs\\\"].shape =\", gradients[\"dbs\"].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Backward pass through the RNN\n",
    "\n",
    "Computing the gradients of the cost with respect to $S_{t}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_S$, $dW_{S}$, $dW_{X}$ and you store $dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for backward pass of RNN\n",
    "def rnn_backward(ds, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - ds: Upstream gradients of all hidden states.\n",
    "          It should be numpy array of shape (n_s, m, T_x)\n",
    "          \n",
    "    -caches: Tuple containing information from the forward pass or rnn_forward()\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - gradients:  A dictionary which contains the following.\n",
    "                     . dx: Gradient w.r.t. the input data.\n",
    "                           It should be numpy array of shape (n_x, m, T_x).\n",
    "                     . ds_0: Gradient w.r.t the initial hidden state.\n",
    "                                It should be numpy array of shape (n_s, m)\n",
    "                     . dWx: Gradient w.r.t the input's weight matrix.\n",
    "                            It should be numpy array of shape (n_s, n_x).\n",
    "                     . dWs: Gradient w.r.t the hidden state's weight matrix.\n",
    "                            It should be numpy array of shape (n_s, n_s).\n",
    "                     . dbs: Gradient w.r.t the bias.\n",
    "                            It should be numpy array of shape (n_s, 1)\n",
    "    \"\"\"\n",
    "    # TODO: Retrieve first cache (t=1) inside caches\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    n_s, m, T_x = # TODO\n",
    "    n_x, m = # TODO\n",
    "    \n",
    "    # Initialize the gradients\n",
    "    dX = # TODO\n",
    "    dWx = # TODO\n",
    "    dWs = # TODO\n",
    "    dbs = # TODO\n",
    "    ds_0 = # TODO\n",
    "    ds_prev = # TODO\n",
    "    \n",
    "\n",
    "    # Loop over all timesteps\n",
    "    for t in reversed(range(T_x)):\n",
    "        \n",
    "        # Compute gradients at time t\n",
    "        gradients = # TODO\n",
    "\n",
    "        # Retrieve derivatives from gradients\n",
    "        dX_t = # TODO\n",
    "        ds_prev = # TODO\n",
    "        dWx_t = # TODO\n",
    "        dWs_t = # TODO\n",
    "        dbs_t = # TODO\n",
    "        \n",
    "        # TODO: Increment global derivatives w.r.t. parameters by adding their derivative at timestep t\n",
    "        \n",
    "        \n",
    "    # Updating ds_0\n",
    "    ds_0 = # TODO\n",
    "    \n",
    "    # Store the gradients\n",
    "    gradients = {\"dX\": dX, \"ds_0\": ds_0, \"dWx\": dWx, \"dWs\": dWs, \"dbs\": dbs}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "X = np.random.randn(3, 10, 4)\n",
    "\n",
    "# Initial hidden state\n",
    "S_0 = np.random.randn(5, 10)\n",
    "\n",
    "# Weight matrices\n",
    "Wx = np.random.randn(5, 3)\n",
    "Ws = np.random.randn(5, 5)\n",
    "Wy = np.random.randn(2, 5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Wx\": Wx, \"Ws\": Ws, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute forward pass of RNN\n",
    "S, y_pred, caches = # TODO\n",
    "\n",
    "# Upstream gradients of all hidden states.\n",
    "ds = np.random.randn(5, 10, 4)\n",
    "\n",
    "# Compute backward pass of RNN\n",
    "gradients = # TODO\n",
    "\n",
    "# Get the shapes\n",
    "print(\"gradients[\\\"dX\\\"].shape =\", gradients[\"dX\"].shape)\n",
    "print(\"gradients[\\\"ds_0\\\"].shape =\", gradients[\"ds_0\"].shape)\n",
    "print(\"gradients[\\\"dWx\\\"].shape =\", gradients[\"dWx\"].shape)\n",
    "print(\"gradients[\\\"dWs\\\"].shape =\", gradients[\"dWs\"].shape)\n",
    "print(\"gradients[\\\"dbs\\\"].shape =\", gradients[\"dbs\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\">Recurrent Neural Networks cheatsheet - Stanford</a>\n",
    "2. <a href=\"https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/\">Fundamentals of Deep Learning â Introduction to Recurrent Neural Networks\n",
    "</a>\n",
    "3. <a href=\"https://blog.usejournal.com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68\">Stock Market Prediction by Recurrent Neural Network on LSTM Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Build Your Own Encrypted Language Using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first sentence and its encripted version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing to our text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the first sentence and its preprocessed version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to our preprocessed text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with a basic RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create your model's architecture\n",
    "\n",
    "# Take a look at model's summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the model\n",
    "\n",
    "# Checkpoint for saving the model\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a funcction for converting logits into text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the first item in tmp_x\n",
    "\n",
    "# Convert the logits into text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\">How to Develop a Character-Based Neural Language Model in Keras</a>\n",
    "2. <a href=\"https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/\">Understanding how to implement a character-based RNN language model</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
