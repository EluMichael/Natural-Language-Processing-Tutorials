{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN) - NumPy\n",
    "\n",
    "A recurrent neural network (RNN) is a class of artificial neural network where connections between nodes form a directed graph along a temporal sequence. This allows it to exhibit temporal dynamic behavior. Unlike feedforward neural networks, RNNs can use their internal state (memory) to process sequences of inputs. This makes them applicable to tasks such as unsegmented, connected handwriting recognition or speech recognition. (wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Implementing activation function of softmax.\n",
    "    \"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Forward propagation for the basic Recurrent Neural NetworkÂ¶\n",
    " A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. RNN cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Recurrent neural network can be seen as the repetition of a single cell. You are first going to implement the computations for a single time-step. The following figure describes the operations for a single time-step of an RNN cell.\n",
    "\n",
    "##### The whole network:\n",
    "<img src = \"./assets/RNN_Cell_1.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>\n",
    " \n",
    "##### Zooming in one cell:\n",
    "<img src = \"./assets/RNN_Cell_2.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>\n",
    " \n",
    "##### One cell in greater detail:\n",
    "<img src = \"./assets/RNN_Cell_3.png\">\n",
    "<p style = \"font-size:10px;color:gray;\">Picture taken from udacity</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for a single forward step for RNN cell\n",
    "def rnn_cell_forward(Xt, St_prev, parameters_dict):\n",
    "    \"\"\"\n",
    "    Implementing a single forward step for the RNN cell.\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    \n",
    "    - Xt: Our input data at timestep \"t\". It should be numpy array of shape (n_x, m).\n",
    "    \n",
    "    - St_prev: Hidden state or state at timestep \"t-1\". It should be numpy array of shape (n_s, m).\n",
    "    \n",
    "    - parameters_dict: Python dictionary containing:\n",
    "                           . Wx: A weight matrix connecting the inputs to the state (or hidden state).\n",
    "                                 It should be numpy array of shape (n_s, n_x).  \n",
    "                           . Ws: A weight matrix connecting the state (or hidden state) from previous timestep to the state (or hidden state) in the current timestep.\n",
    "                                 It should be numpy array of shape (n_s, n_s).\n",
    "                           . Wy: A weight matrix connecting the state (or hidden state) to the output layer.\n",
    "                                 It should be numpy array of shape (n_y, n_s).\n",
    "                           . bs: Bias for state. \n",
    "                                 It should be numpy array of shape (n_s, 1).\n",
    "                           . by: Bias for output layer. \n",
    "                                 It should be numpy array of shape (n_y, 1).\n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - St_next: Next state (or hidden state).\n",
    "               It should be numpy array of shape (n_s, m).\n",
    "               \n",
    "    - Yt: Prediction at timestep \"t\".\n",
    "          It should be numpy array of shape (n_y, m).\n",
    "          \n",
    "    - cache: Tuple of values needed for the backward pass. contains (St_next, St_prev, Xt, parameters_dict)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrive parameters\n",
    "    Wx = parameters_dict['Wx']\n",
    "    Ws = parameters_dict['Ws']\n",
    "    Wy = parameters_dict['Wy']\n",
    "    bs = parameters_dict['bs']\n",
    "    by = parameters_dict['by']\n",
    "    \n",
    "    # Compute the next activation state\n",
    "    St_next = np.tanh(np.dot(Wx, Xt) + np.dot(Ws, St_prev) + bs)\n",
    "    \n",
    "    # Compute output of current cell\n",
    "    Yt = softmax(np.dot(Wy, St_next) + by)\n",
    "    \n",
    "    # Store variables for backpropagation\n",
    "    cache = (St_next, St_prev, Xt, parameters_dict)\n",
    "    \n",
    "    return St_next, Yt, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden state at time t:  (5, 10) \n",
      "\n",
      "Output shape:  (2, 10)\n"
     ]
    }
   ],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "Xt = np.random.randn(3, 10)\n",
    "\n",
    "# Hidden state at time t-1\n",
    "St_prev = np.random.randn(5, 10)\n",
    "\n",
    "# Weight matrices\n",
    "Ws = np.random.randn(5, 5)\n",
    "Wx = np.random.randn(5, 3)\n",
    "Wy = np.random.randn(2, 5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Ws\": Ws, \"Wx\": Wx, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute the forward step for the RNN cell.\n",
    "St_next, Yt, cache = rnn_cell_forward(Xt = Xt, \n",
    "                                      St_prev = St_prev, \n",
    "                                      parameters_dict = parameters)\n",
    "\n",
    "# Get the shape of hidden state at time t\n",
    "print(\"Shape of hidden state at time t: \", St_next.shape, \"\\n\")\n",
    "\n",
    "# Get the shape of output\n",
    "print(\"Output shape: \", Yt.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. RNN forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see an RNN as the repetition of the cell you've just built. If your input sequence of data is carried over 10 time steps, then you will copy the RNN cell 10 times. Each cell takes as input the hidden state from the previous cell and the current time-step's input data. It outputs a hidden state and a prediction for this time-step.\n",
    "\n",
    "<img src = \"./assets/RNN.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for forward pass od RNN\n",
    "def rnn_forward(X, S_0, parameters_dict):\n",
    "    \"\"\"\n",
    "    Implementing the forward pass of the recurrent neural network (RNN).\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - X: Input data for every time-step.\n",
    "         It should be numpy array of shape (n_x, m, T_x). T_x is total length of x.\n",
    "         \n",
    "    - S_initial: Initial hidden state.\n",
    "                 It should be numpy array of shape (n_s, m).\n",
    "                 \n",
    "    - parameters_dict: Python dictionary containing:\n",
    "                           . Wx: A weight matrix connecting the inputs to the state (or hidden state).\n",
    "                                 It should be numpy array of shape (n_s, n_x).  \n",
    "                           . Ws: A weight matrix connecting the state (or hidden state) from previous timestep to the state (or hidden state) in the current timestep.\n",
    "                                 It should be numpy array of shape (n_s, n_s).\n",
    "                           . Wy: A weight matrix connecting the state (or hidden state) to the output layer.\n",
    "                                 It should be numpy array of shape (n_y, n_s).\n",
    "                           . bs: Bias for state. \n",
    "                                 It should be numpy array of shape (n_s, 1).\n",
    "                           . by: Bias for output layer. \n",
    "                                 It should be numpy array of shape (n_y, 1).\n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - S: Hidden states for every time-step.\n",
    "         It should be numpy array of shape (n_s, m, T_x).\n",
    "         \n",
    "    - y_pred: Predictions for every time-step.\n",
    "              It should be numpy array of shape (n_y, m, T_x).\n",
    "          \n",
    "    - caches: Tuple of values needed for the backward pass, contains (list of caches, x)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrive dimensions\n",
    "    n_x, m, T_x = X.shape\n",
    "    n_y, n_s = parameters_dict['Wy'].shape\n",
    "    \n",
    "    # Initialize state \"S\" and output \"y\" with zeros\n",
    "    S = np.zeros(shape = (n_s, m, T_x))\n",
    "    y_pred = np.zeros(shape = (n_y, m, T_x))\n",
    "    \n",
    "    # Initialize S_next\n",
    "    S_next = S_0\n",
    "    \n",
    "    # Initialize caches which contains all caches\n",
    "    caches = []\n",
    "    \n",
    "    # Loop over all timesteps\n",
    "    for t in range(T_x):\n",
    "        \n",
    "        # Updating next state, Predicting the output, And getting the cache\n",
    "        S_next, Yt, cache = rnn_cell_forward(Xt = X[:, :, t],\n",
    "                                              St_prev = S_next,\n",
    "                                              parameters_dict = parameters_dict)\n",
    "        \n",
    "        # Save the next state into hidden state\n",
    "        S[:, :, t] = S_next\n",
    "        \n",
    "        # Save the value of prediction into y\n",
    "        y_pred[:, :, t] = Yt\n",
    "        \n",
    "        # Append \"cache\" into \"caches\"\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Store values needed for backward propagation in cache\n",
    "    caches = (caches, X)\n",
    "    \n",
    "    return S, y_pred, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of hidden state: (5, 10, 4) \n",
      "\n",
      "Shape of predicted y:  (2, 10, 4) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "X = np.random.randn(3,10,4)\n",
    "\n",
    "# Initial hidden state\n",
    "S_0 = np.random.randn(5,10)\n",
    "\n",
    "# Weight matrices\n",
    "Ws = np.random.randn(5,5)\n",
    "Wx = np.random.randn(5,3)\n",
    "Wy = np.random.randn(2,5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Ws\": Ws, \"Wx\": Wx, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute forward pass of the RNN\n",
    "S, y_pred, caches = rnn_forward(X = X, \n",
    "                                S_0 = S_0, \n",
    "                                parameters_dict = parameters)\n",
    "\n",
    "# Get the shape of hidden state\n",
    "print(\"Shape of hidden state:\", S.shape, \"\\n\")\n",
    "\n",
    "# Get the shape of predicted y\n",
    "print(\"Shape of predicted y: \", y_pred.shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Backpropagation in recurrent neural networks\n",
    "\n",
    "In modern deep learning frameworks, we only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers do not need to bother with the details of the backward pass. \n",
    "\n",
    "In a simple (fully connected) neural network, we did backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in recurrent neural networks we can to calculate the derivatives with respect to the cost in order to update the parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style = \"width:650px\" src = \"./assets/BBTT.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Basic RNN cell backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the backward pass of RNN cell\n",
    "def rnn_cell_backward(ds_next, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for RNN cell (single timestep)\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - ds_next: Gradient of loss with respect to next hidden state.\n",
    "    \n",
    "    - cache: A dictionary containing the output of rnn_cell_forward()\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - gradients: A dictionary which contains the following.\n",
    "                     . dx: Gradients of input data.\n",
    "                           It should be numpy array of shape (n_x, m)\n",
    "                     . ds_prev: Gradients of previous hidden state.\n",
    "                                It should be numpy array of shape (n_s, m)\n",
    "                     . dWx: Gradients of input-to-hidden weights. \n",
    "                            It should be numpy array of shape (n_s, n_x).\n",
    "                     . dWs: Gradients of hidden-to-hidden weights.\n",
    "                            It should be numpy array of shape (n_s, n_s).\n",
    "                     . dbs: Gradients of bias vector. \n",
    "                            It should be numpy array of shape (n_s, 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    # Retrieve values from cache\n",
    "    (S_next, S_prev, Xt, parameters_dict) = cache\n",
    "    \n",
    "    # Retrieve values from parameters\n",
    "    Wx = parameters_dict[\"Wx\"]\n",
    "    Ws = parameters_dict[\"Ws\"]\n",
    "    Wy = parameters_dict[\"Wy\"]\n",
    "    bs = parameters_dict[\"bs\"]\n",
    "    by = parameters_dict[\"by\"]\n",
    "    \n",
    "    # Compute the gradient of tanh with respect to S_next\n",
    "    dtanh = (1 - S_next ** 2) * ds_next\n",
    "    \n",
    "    # Compute the gradient of loss with repect to Wx\n",
    "    dXt = np.dot(Wx.T, dtanh)\n",
    "    dWx = np.dot(dtanh, Xt.T)\n",
    "    \n",
    "    # Compute the gradient with respect to Ws\n",
    "    ds_prev = np.dot(Ws.T, dtanh)\n",
    "    dWs = np.dot(dtanh, S_prev.T)\n",
    "    \n",
    "    # Compute the gradient with respect b\n",
    "    dbs = np.sum(dtanh, axis = 1, keepdims = 1)\n",
    "    \n",
    "    # Store the gradients\n",
    "    gradients = {\"dXt\": dXt, \"ds_prev\": ds_prev, \"dWx\": dWx, \"dWs\": dWs, \"dbs\": dbs}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dXt\"].shape = (3, 10) \n",
      "\n",
      "gradients[\"ds_prev\"].shape = (5, 10) \n",
      "\n",
      "gradients[\"dWx\"].shape = (5, 3) \n",
      "\n",
      "gradients[\"dWs\"].shape = (5, 5) \n",
      "\n",
      "gradients[\"dbs\"].shape = (5, 1) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "Xt = np.random.randn(3,10)\n",
    "\n",
    "# Previous hidden state\n",
    "S_prev = np.random.randn(5,10)\n",
    "\n",
    "# Weight matrices\n",
    "Wx = np.random.randn(5,3)\n",
    "Ws = np.random.randn(5,5)\n",
    "Wy = np.random.randn(2,5)\n",
    "\n",
    "# Biases\n",
    "b = np.random.randn(5,1)\n",
    "by = np.random.randn(2,1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Wx\": Wx, \"Ws\": Ws, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute single forward step for RNN cell\n",
    "S_next, Yt, cache = rnn_cell_forward(Xt, S_prev, parameters)\n",
    "\n",
    "# Gradient of loss with respect to next hidden state.\n",
    "ds_next = np.random.randn(5,10)\n",
    "\n",
    "# Compute backward pass of the RNN cell\n",
    "gradients = rnn_cell_backward(ds_next, cache)\n",
    "\n",
    "# Get the shapes\n",
    "print(\"gradients[\\\"dXt\\\"].shape =\", gradients[\"dXt\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"ds_prev\\\"].shape =\", gradients[\"ds_prev\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dWx\\\"].shape =\", gradients[\"dWx\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dWs\\\"].shape =\", gradients[\"dWs\"].shape, \"\\n\")\n",
    "print(\"gradients[\\\"dbs\\\"].shape =\", gradients[\"dbs\"].shape, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Backward pass through the RNN\n",
    "\n",
    "Computing the gradients of the cost with respect to $S_{t}$ at every time-step $t$ is useful because it is what helps the gradient backpropagate to the previous RNN-cell. To do so, you need to iterate through all the time steps starting at the end, and at each step, you increment the overall $db_S$, $dW_{S}$, $dW_{X}$ and you store $dx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for backward pass of RNN\n",
    "def rnn_backward(ds, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward pass for a RNN over an entire sequence of input data.\n",
    "    \n",
    "    Arguments\n",
    "    -------------------------------------------------------------------------\n",
    "    - ds: Upstream gradients of all hidden states.\n",
    "          It should be numpy array of shape (n_s, m, T_x)\n",
    "          \n",
    "    -caches: Tuple containing information from the forward pass or rnn_forward()\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------------------------------------------------------------------------\n",
    "    - gradients:  A dictionary which contains the following.\n",
    "                     . dx: Gradient w.r.t. the input data.\n",
    "                           It should be numpy array of shape (n_x, m, T_x).\n",
    "                     . ds_0: Gradient w.r.t the initial hidden state.\n",
    "                                It should be numpy array of shape (n_s, m)\n",
    "                     . dWx: Gradient w.r.t the input's weight matrix.\n",
    "                            It should be numpy array of shape (n_s, n_x).\n",
    "                     . dWs: Gradient w.r.t the hidden state's weight matrix.\n",
    "                            It should be numpy array of shape (n_s, n_s).\n",
    "                     . dbs: Gradient w.r.t the bias.\n",
    "                            It should be numpy array of shape (n_s, 1)\n",
    "    \"\"\"\n",
    "    # Retrieve first cache (t=1) inside caches\n",
    "    (caches, X) = caches\n",
    "    (S1, S0, X1, parameters) = caches[0]\n",
    "    \n",
    "    # Retrieve dimensions\n",
    "    n_s, m, T_x = ds.shape\n",
    "    n_x, m = X1.shape\n",
    "    \n",
    "    # Initialize the gradients\n",
    "    dX = np.zeros(shape = (n_x, m, T_x))\n",
    "    dWx = np.zeros(shape = (n_s, n_x))\n",
    "    dWs = np.zeros(shape = (n_s, n_s))\n",
    "    dbs = np.zeros(shape = (n_s, 1))\n",
    "    ds_0 = np.zeros(shape = (n_s, m))\n",
    "    ds_prev = np.zeros(shape = (n_s, m))\n",
    "    \n",
    "\n",
    "    # Loop over all timesteps\n",
    "    for t in reversed(range(T_x)):\n",
    "        \n",
    "        # Compute gradients at time t\n",
    "        gradients = rnn_cell_backward(ds_next = ds[:, :, t] + ds_prev,\n",
    "                                      cache = caches[t])\n",
    "\n",
    "        # Retrieve derivatives from gradients\n",
    "        dX_t = gradients['dXt']\n",
    "        ds_prev = gradients['ds_prev']\n",
    "        dWx_t = gradients['dWx']\n",
    "        dWs_t = gradients['dWs']\n",
    "        dbs_t = gradients['dbs']\n",
    "        \n",
    "        # Increment global derivatives w.r.t. parameters by adding their derivative at timestep t\n",
    "        dX[:, :, t] = dX_t\n",
    "        dWx += dWx_t\n",
    "        dWs += dWs_t\n",
    "        dbs += dbs_t\n",
    "        \n",
    "    # Updating ds_0\n",
    "    ds_0 = ds_prev\n",
    "    \n",
    "    # Store the gradients\n",
    "    gradients = {\"dX\": dX, \"ds_0\": ds_0, \"dWx\": dWx, \"dWs\": dWs, \"dbs\": dbs}\n",
    "    \n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradients[\"dX\"].shape = (3, 10, 4)\n",
      "gradients[\"ds_0\"].shape = (5, 10)\n",
      "gradients[\"dWx\"].shape = (5, 3)\n",
      "gradients[\"dWs\"].shape = (5, 5)\n",
      "gradients[\"dbs\"].shape = (5, 1)\n"
     ]
    }
   ],
   "source": [
    "### Testing\n",
    "\n",
    "# Random seed\n",
    "np.random.seed(1)\n",
    "\n",
    "# Input data\n",
    "X = np.random.randn(3, 10, 4)\n",
    "\n",
    "# Initial hidden state\n",
    "S_0 = np.random.randn(5, 10)\n",
    "\n",
    "# Weight matrices\n",
    "Wx = np.random.randn(5, 3)\n",
    "Ws = np.random.randn(5, 5)\n",
    "Wy = np.random.randn(2, 5)\n",
    "\n",
    "# Biases\n",
    "bs = np.random.randn(5, 1)\n",
    "by = np.random.randn(2, 1)\n",
    "\n",
    "# Storing all parameters inside a dictionary\n",
    "parameters = {\"Wx\": Wx, \"Ws\": Ws, \"Wy\": Wy, \"bs\": bs, \"by\": by}\n",
    "\n",
    "# Compute forward pass of RNN\n",
    "S, y_pred, caches = rnn_forward(X, S_0, parameters)\n",
    "\n",
    "# Upstream gradients of all hidden states.\n",
    "ds = np.random.randn(5, 10, 4)\n",
    "\n",
    "# Compute backward pass of RNN\n",
    "gradients = rnn_backward(ds, caches)\n",
    "\n",
    "# Get the shapes\n",
    "print(\"gradients[\\\"dX\\\"].shape =\", gradients[\"dX\"].shape)\n",
    "print(\"gradients[\\\"ds_0\\\"].shape =\", gradients[\"ds_0\"].shape)\n",
    "print(\"gradients[\\\"dWx\\\"].shape =\", gradients[\"dWx\"].shape)\n",
    "print(\"gradients[\\\"dWs\\\"].shape =\", gradients[\"dWs\"].shape)\n",
    "print(\"gradients[\\\"dbs\\\"].shape =\", gradients[\"dbs\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks\">Recurrent Neural Networks cheatsheet - Stanford</a>\n",
    "2. <a href=\"https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/\">Fundamentals of Deep Learning â Introduction to Recurrent Neural Networks\n",
    "</a>\n",
    "3. <a href=\"https://blog.usejournal.com/stock-market-prediction-by-recurrent-neural-network-on-lstm-model-56de700bff68\">Stock Market Prediction by Recurrent Neural Network on LSTM Model</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# Build Your Own Encrypted Language Using RNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "codes = helper.load_data('./dataset/cipher.txt')\n",
    "plaintext = helper.load_data('./dataset/plaintext.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence:  THE LIME IS HER LEAST LIKED FRUIT , BUT THE BANANA IS MY LEAST LIKED .\n",
      "Encripted:  YMJ QNRJ NX MJW QJFXY QNPJI KWZNY , GZY YMJ GFSFSF NX RD QJFXY QNPJI .\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first sentence and its encripted version\n",
    "print(\"Sentence: \", plaintext[0])\n",
    "print(\"Encripted: \", codes[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_text(list_of_strings):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    char_tokens = Tokenizer(lower = True,\n",
    "                            split = ' ',\n",
    "                            char_level = True,\n",
    "                            filters = '!\"#$%&()*+,-./:;<=>?@[\\\\\\\\]^_`{|}~\\\\t\\\\n\\'')\n",
    "    \n",
    "    # Fitting the tokenizer to our text\n",
    "    char_tokens.fit_on_texts(list_of_strings)\n",
    "    \n",
    "    return char_tokens.texts_to_sequences(list_of_strings), char_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the preprocessing to our text\n",
    "codes_preprocessed, codes_tokenizer = preprocess_text(codes)\n",
    "plaintext_preprocessed, plaintext_tokenizer = preprocess_text(plaintext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: \n",
      " THE LIME IS HER LEAST LIKED FRUIT , BUT THE BANANA IS MY LEAST LIKED . \n",
      "\n",
      "Preprocessed Sentence: \n",
      " [5, 14, 3, 1, 10, 2, 13, 3, 1, 2, 4, 1, 14, 3, 6, 1, 10, 3, 8, 4, 5, 1, 10, 2, 25, 3, 11, 1, 20, 6, 9, 2, 5, 1, 18, 1, 17, 9, 5, 1, 5, 14, 3, 1, 17, 8, 7, 8, 7, 8, 1, 2, 4, 1, 13, 15, 1, 10, 3, 8, 4, 5, 1, 10, 2, 25, 3, 11, 1, 19]\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the first sentence and its preprocessed version\n",
    "print(\"Sentence: \\n\", plaintext[0], \"\\n\")\n",
    "print(\"Preprocessed Sentence: \\n\", plaintext_preprocessed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding function\n",
    "def padding(list_of_strings, length = None):\n",
    "    \n",
    "    # Define length, If it is not defined\n",
    "    if length == None:\n",
    "        length = max([len(i_text) for i_text in list_of_strings])\n",
    "        \n",
    "    # Add padding\n",
    "    pad = pad_sequences(list_of_strings, maxlen = length, padding = \"post\")\n",
    "    \n",
    "    return pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to our preprocessed text\n",
    "codes_preprocessed = padding(codes_preprocessed)\n",
    "plaintext_preprocessed = padding(plaintext_preprocessed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras's sparse_categorical_crossentropy function requires the labels to be in 3 dimensions\n",
    "plaintext_preprocessed = plaintext_preprocessed.reshape(*plaintext_preprocessed.shape, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create a Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "learning_rate = 1e-3\n",
    "gru_units = 64\n",
    "batch_size=32 \n",
    "epochs=5 \n",
    "validation_split=0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the input to work with a basic RNN\n",
    "tmp_x = padding(list_of_strings = codes_preprocessed, \n",
    "                length = plaintext_preprocessed.shape[1])\n",
    "\n",
    "tmp_x = tmp_x.reshape((-1, plaintext_preprocessed.shape[-2], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 101, 1)            0         \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 101, 64)           12672     \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 101, 32)           2080      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 101, 32)           0         \n",
      "=================================================================\n",
      "Total params: 14,752\n",
      "Trainable params: 14,752\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create your model's architecture\n",
    "input_model = Input(tmp_x.shape[1:])\n",
    "rnn_1 = GRU(units = gru_units, return_sequences = True)(input_model)\n",
    "logits = TimeDistributed(Dense(len(plaintext_tokenizer.word_index) + 1))(rnn_1)\n",
    "model = Model(input_model, Activation(\"softmax\")(logits))\n",
    "\n",
    "# Take a look at model's summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss = sparse_categorical_crossentropy,\n",
    "              optimizer = Adam(learning_rate),\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2001 samples\n",
      "Epoch 1/5\n",
      "8000/8000 [==============================] - 35s 4ms/step - loss: 1.5884 - acc: 0.5737 - val_loss: 0.9569 - val_acc: 0.7405\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.95687, saving model to ./saved model/weights.best.deciphering.hdf5\n",
      "Epoch 2/5\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.7349 - acc: 0.8089 - val_loss: 0.5694 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.95687 to 0.56940, saving model to ./saved model/weights.best.deciphering.hdf5\n",
      "Epoch 3/5\n",
      "8000/8000 [==============================] - 32s 4ms/step - loss: 0.4520 - acc: 0.8944 - val_loss: 0.3615 - val_acc: 0.9215\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.56940 to 0.36154, saving model to ./saved model/weights.best.deciphering.hdf5\n",
      "Epoch 4/5\n",
      "8000/8000 [==============================] - 34s 4ms/step - loss: 0.2974 - acc: 0.9377 - val_loss: 0.2466 - val_acc: 0.9535\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36154 to 0.24660, saving model to ./saved model/weights.best.deciphering.hdf5\n",
      "Epoch 5/5\n",
      "8000/8000 [==============================] - 33s 4ms/step - loss: 0.2085 - acc: 0.9605 - val_loss: 0.1763 - val_acc: 0.9668\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.24660 to 0.17632, saving model to ./saved model/weights.best.deciphering.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11145b6a0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "# Checkpoint for saving the model\n",
    "checkpointer = ModelCheckpoint(filepath='./saved model/weights.best.deciphering.hdf5', \n",
    "                               verbose = 1, \n",
    "                               save_best_only = True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(tmp_x, \n",
    "          plaintext_preprocessed,\n",
    "          batch_size = batch_size,\n",
    "          epochs = epochs,\n",
    "          validation_split = validation_split,\n",
    "          callbacks = [checkpointer], \n",
    "          verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a funcction for converting logits into text\n",
    "def logits_to_text(logits, tokenizer):\n",
    "    \n",
    "    # Get index to words\n",
    "    index2word = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    \n",
    "    # Add '<PAD>' at start of index2word\n",
    "    index2word[0] = '<PAD>'\n",
    "    \n",
    "    # Get the text\n",
    "    text = \"\".join([index2word[prediction] for prediction in np.argmax(logits, 1)])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the lime is her least liked fruit , but the banana is my least liked .<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "# Predict the first item in tmp_x\n",
    "prediction = model.predict(tmp_x[:1])[0]\n",
    "\n",
    "# Convert the logits into text\n",
    "predicted_text = logits_to_text(logits = prediction, \n",
    "                                tokenizer = plaintext_tokenizer)\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**RESOURCES:**\n",
    "1. <a href=\"https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/\">How to Develop a Character-Based Neural Language Model in Keras</a>\n",
    "2. <a href=\"https://eli.thegreenplace.net/2018/understanding-how-to-implement-a-character-based-rnn-language-model/\">Understanding how to implement a character-based RNN language model</a>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
